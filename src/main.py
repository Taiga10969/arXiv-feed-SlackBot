#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
arXiv æ–°ç€è«–æ–‡é€šçŸ¥ãƒœãƒƒãƒˆ

ç‰¹å¾´:
- æŒ‡å®šã•ã‚ŒãŸarXivã‚«ãƒ†ã‚´ãƒªã‹ã‚‰æ–°ç€è«–æ–‡ã‚’è‡ªå‹•å–å¾—
- ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹é–¢é€£è«–æ–‡ã®æŠ½å‡º
- ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ï¼ˆã‚¿ã‚¤ãƒˆãƒ«Ã—2 + è¦ç´„Ã—1ï¼‰
- é‡è¤‡é€šçŸ¥ã®é˜²æ­¢ï¼ˆæ—¢èª­ç®¡ç†ï¼‰
- Slackã¸ã®è‡ªå‹•é€šçŸ¥
- ç¿»è¨³æ©Ÿèƒ½ï¼ˆGoogle Cloud Translation APIï¼‰

ä½¿ç”¨æ–¹æ³•:
  python src/main.py --config configs/config.yaml
"""

import os
import re
import json
import zoneinfo
import datetime as dt
import xml.etree.ElementTree as ET
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional

import argparse
import requests
import yaml

# ==============================
# è¨­å®š & æ—¢èª­ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
# ==============================
def load_config_and_state():
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã¨æ—¢èª­çŠ¶æ…‹ã‚’èª­ã¿è¾¼ã¿"""
    parser = argparse.ArgumentParser(description="arXivæ–°ç€è«–æ–‡é€šçŸ¥ãƒœãƒƒãƒˆ")
    parser.add_argument(
        "--config",
        type=str,
        default=os.environ.get("CONFIG_PATH", "configs/config.yaml"),
        help="è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹",
    )
    args = parser.parse_args()

    cfg_path = Path(args.config)
    base_dir = Path(__file__).resolve().parent.parent

    # ãƒ¡ã‚¤ãƒ³è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    config = yaml.safe_load(cfg_path.read_text(encoding="utf-8"))
    
    # å¤–éƒ¨è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
    # ã‚«ãƒ†ã‚´ãƒªè¨­å®š
    categories_file = config.get("categories_file", "configs/categories.yaml")
    categories_path = base_dir / categories_file
    if categories_path.exists():
        categories_config = yaml.safe_load(categories_path.read_text(encoding="utf-8"))
        config["categories"] = categories_config.get("categories", ["cs.CV"])
    else:
        print(f"[WARN] Categories file not found: {categories_path}")
        config["categories"] = ["cs.CV"]
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¨­å®š
    keywords_file = config.get("keywords_file", "configs/keywords.yaml")
    keywords_path = base_dir / keywords_file
    if keywords_path.exists():
        keywords_config = yaml.safe_load(keywords_path.read_text(encoding="utf-8"))
        config["keywords"] = keywords_config.get("keywords", [])
    else:
        print(f"[WARN] Keywords file not found: {keywords_path}")
        config["keywords"] = []

    # æ—¢èª­ç®¡ç†ãƒ•ã‚¡ã‚¤ãƒ«ã®è¨­å®š
    state_file = config.get("state_file", "seen.json")
    seen_path = base_dir / "data" / state_file
    seen_path.parent.mkdir(parents=True, exist_ok=True)
    if not seen_path.exists():
        seen_path.write_text("[]", encoding="utf-8")

    # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ãƒ»ç¾åœ¨æ™‚åˆ»
    tz = zoneinfo.ZoneInfo(config.get("timezone", "Asia/Tokyo"))
    now_local = dt.datetime.now(tz)

    # æ—¢èª­IDã‚»ãƒƒãƒˆ
    try:
        seen = set(json.loads(seen_path.read_text(encoding="utf-8")))
    except Exception:
        seen = set()
        seen_path.write_text("[]", encoding="utf-8")

    return config, base_dir, cfg_path, seen_path, seen, tz, now_local


# ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®šï¼ˆä¸€åº¦ã ã‘ãƒ­ãƒ¼ãƒ‰ï¼‰
CONFIG, BASE_DIR, CFG_PATH, SEEN_PATH, SEEN, TZ, NOW_LOCAL = load_config_and_state()


# ==============================
# arXiv å–å¾— (Atom API)
# ==============================
ARXIV_ATOM = "http://export.arxiv.org/api/query"

def fetch_arxiv(categories: List[str], max_results: int = 200) -> List[Dict[str, Any]]:
    """æŒ‡å®šã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªã‹ã‚‰arXivè«–æ–‡ã‚’å–å¾—"""
    cat_query = " OR ".join([f"cat:{c}" for c in categories])
    params = {
        "search_query": cat_query,
        "start": 0,
        "max_results": max_results,
        "sortBy": "submittedDate",
        "sortOrder": "descending",
    }
    
    try:
        r = requests.get(ARXIV_ATOM, params=params, timeout=30)
        r.raise_for_status()
        root = ET.fromstring(r.text)
        ns = {"ns": "http://www.w3.org/2005/Atom"}

        items: List[Dict[str, Any]] = []
        for entry in root.findall("ns:entry", ns):
            title = (entry.find("ns:title", ns).text or "").strip()
            summary = (entry.find("ns:summary", ns).text or "").strip()
            link = (entry.find("ns:id", ns).text or "").strip()
            published = (entry.find("ns:published", ns).text or "").strip()
            updated = (entry.find("ns:updated", ns).text or "").strip()
            arxiv_id = link.rsplit("/", 1)[-1]
            
            items.append({
                "id": arxiv_id,
                "title": title.replace("\n", " "),
                "summary": summary.replace("\n", " "),
                "link": link,
                "published": published,
                "updated": updated,
            })
        
        print(f"[INFO] Fetched {len(items)} papers from arXiv")
        return items
        
    except Exception as e:
        print(f"[ERROR] Failed to fetch from arXiv: {e}")
        return []


# ==============================
# ãƒ•ã‚£ãƒ«ã‚¿ & ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
# ==============================
def within_search_hours(iso8601_str: str, hours_back: int) -> bool:
    """æŒ‡å®šæ™‚é–“ä»¥å†…ã«å…¬é–‹ã•ã‚ŒãŸè«–æ–‡ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
    try:
        t = dt.datetime.fromisoformat(iso8601_str.replace("Z", "+00:00"))
        now_utc = dt.datetime.now(dt.timezone.utc)
        return (now_utc - t).total_seconds() <= hours_back * 3600
    except Exception:
        return False

def parse_iso8601(s: str) -> dt.datetime:
    """ISO8601æ–‡å­—åˆ—ã‚’datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›"""
    return dt.datetime.fromisoformat(s.replace("Z", "+00:00"))

def compile_kw_patterns(kw_list: List[str]) -> List[re.Pattern]:
    """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã‹ã‚‰æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½œæˆ"""
    if not kw_list:
        return []
    
    patterns: List[re.Pattern] = []
    for kw in kw_list:
        if "|" in kw:
            patterns.append(re.compile(kw, re.IGNORECASE))
        else:
            patterns.append(re.compile(re.escape(kw), re.IGNORECASE))
    return patterns

def compute_match_score(title: str, summary: str, patterns: List[re.Pattern]) -> Tuple[int, List[str]]:
    """ä¸€è‡´ã‚¹ã‚³ã‚¢ã¨ãƒãƒƒãƒã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¨ˆç®—"""
    if not patterns:
        return 0, []
    
    score_title = 0
    score_summary = 0
    matched_keywords = set()
    
    for pattern in patterns:
        title_matches = pattern.findall(title)
        summary_matches = pattern.findall(summary)
        
        if title_matches:
            score_title += len(title_matches)
            matched_keywords.add(pattern.pattern)
        if summary_matches:
            score_summary += len(summary_matches)
            matched_keywords.add(pattern.pattern)
    
    return score_title * 2 + score_summary * 1, list(matched_keywords)

def select_by_relevance(
    items: List[Dict[str, Any]],
    kw_patterns: List[re.Pattern],
    max_posts: int,
) -> List[Tuple[Dict[str, Any], List[str]]]:
    """é–¢é€£æ€§ã«åŸºã¥ã„ã¦è«–æ–‡ã‚’é¸æŠ"""
    candidates: List[Tuple[int, dt.datetime, Dict[str, Any], List[str]]] = []
    
    # æ¤œç´¢æ™‚é–“ã®è¨­å®šã‚’å–å¾—
    hours_back = CONFIG.get("search", {}).get("hours_back", 24)
    
    for item in items:
        if item["id"] in SEEN:
            continue
        if not within_search_hours(item["published"], hours_back):
            continue

        if kw_patterns:
            score, matched_kw = compute_match_score(item["title"], item["summary"], kw_patterns)
            if score <= 0:
                continue
        else:
            score = 0
            matched_kw = []

        pub_dt = parse_iso8601(item["published"])
        candidates.append((score, pub_dt, item, matched_kw))

    if not candidates:
        return []

    # ã‚¹ã‚³ã‚¢é™é † â†’ å…¬é–‹æ—¥æ™‚é™é †
    candidates.sort(key=lambda x: (x[0], x[1]), reverse=True)
    
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’å‡ºåŠ›
    print(f"[DEBUG] select_by_relevance: {len(candidates)} candidates found")
    print(f"[DEBUG] select_by_relevance: max_posts = {max_posts}")
    print(f"[DEBUG] select_by_relevance: returning {min(len(candidates), max_posts)} papers")
    
    return [(item, matched_kw) for _, _, item, matched_kw in candidates[:max_posts]]


# ==============================
# ç¿»è¨³æ©Ÿèƒ½
# ==============================
def maybe_translate(text: str) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’ç¿»è¨³ï¼ˆGoogle Cloud Translation APIä½¿ç”¨ï¼‰"""
    tr_cfg = CONFIG.get("translate", {})
    if not tr_cfg.get("enabled", False):
        return text
    
    try:
        from google.cloud import translate_v2 as translate
        client = translate.Client()
        res = client.translate(text, target_language=tr_cfg.get("target_language", "ja"))
        return res["translatedText"]
    except Exception as e:
        print(f"[WARN] Translation failed: {e}")
        return text


# ==============================
# Slack é€šçŸ¥
# ==============================
def post_to_slack_webhook(blocks: List[Dict[str, Any]]) -> None:
    """Slack Webhookã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡"""
    url = os.environ.get("SLACK_WEBHOOK_URL")
    if not url:
        raise RuntimeError("SLACK_WEBHOOK_URL is not set")
    
    payload = {
        "username": CONFIG.get("slack", {}).get("username", "arXiv Bot"),
        "blocks": blocks,
    }
    
    # ã‚¢ã‚¤ã‚³ãƒ³è¨­å®š
    slack_config = CONFIG.get("slack", {})
    if slack_config.get("icon_url"):
        payload["icon_url"] = slack_config["icon_url"]
    elif slack_config.get("icon_emoji"):
        payload["icon_emoji"] = slack_config["icon_emoji"]
    else:
        payload["icon_emoji"] = ":newspaper:"
    
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
    payload_size = len(json.dumps(payload))
    print(f"[DEBUG] Payload size: {payload_size} characters")
    
    if payload_size > 50000:
        print(f"[WARN] Payload size ({payload_size}) exceeds Slack's 50KB limit")
    
    try:
        r = requests.post(url, json=payload, timeout=30)
        r.raise_for_status()
        print(f"[INFO] Successfully posted to Slack (status: {r.status_code})")
    except requests.exceptions.HTTPError as e:
        print(f"[ERROR] Slack HTTP error: {e}")
        print(f"[ERROR] Response status: {r.status_code}")
        print(f"[ERROR] Response body: {r.text}")
        
        if "invalid_blocks" in r.text:
            print(f"[ERROR] Invalid blocks error detected. Checking block structure...")
            for i, block in enumerate(payload["blocks"]):
                print(f"[ERROR] Block {i}: {json.dumps(block, ensure_ascii=False)}")
        
        raise
    except Exception as e:
        print(f"[ERROR] Unexpected error posting to Slack: {e}")
        raise

def clean_text_for_slack(text: str) -> str:
    """Slackç”¨ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    # åˆ¶å¾¡æ–‡å­—ã‚’é™¤å»ï¼ˆæ”¹è¡Œã¨ã‚¿ãƒ–ã¯ä¿æŒï¼‰
    cleaned = "".join(char for char in text if ord(char) >= 32 or char in "\n\t")
    # å•é¡Œã®ã‚ã‚‹æ–‡å­—ã‚’é™¤å»
    cleaned = cleaned.replace("\x7F", "").replace("\x80", "").replace("\x81", "")
    return cleaned

def make_slack_blocks(entries: List[Tuple[Dict[str, Any], List[str]]], total_count: int = None, displayed_count: int = None) -> List[Dict[str, Any]]:
    """Slackãƒ–ãƒ­ãƒƒã‚¯ã‚’ä½œæˆ"""
    try:
        blocks: List[Dict[str, Any]] = []
        
        # æ—¥ä»˜ã®å®‰å…¨ãªå‡¦ç†
        try:
            date_str = NOW_LOCAL.strftime('%Y-%m-%d')
        except Exception:
            date_str = "ä»Šæ—¥"
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã®ä½œæˆï¼ˆç·ä»¶æ•°æƒ…å ±ã‚’å«ã‚€ï¼‰
        if total_count and displayed_count and total_count > displayed_count:
            header_text = f"arXiv ã§å…¬é–‹ã•ã‚ŒãŸæ–°ç€è«–æ–‡ ({date_str}) - å…¨{total_count}ä»¶ä¸­{displayed_count}ä»¶è¡¨ç¤º"
        else:
            header_text = f"arXiv ã§å…¬é–‹ã•ã‚ŒãŸæ–°ç€è«–æ–‡ ({date_str})"
        
        header = {
            "type": "header",
            "text": {
                "type": "plain_text",
                "text": header_text,
                "emoji": True,
            },
        }
        blocks.append(header)

        # è¡¨ç¤ºè¨­å®šã‚’å–å¾—
        display_config = CONFIG.get("display", {})
        show_keywords = display_config.get("show_keywords", True)
        show_abstract = display_config.get("show_abstract", False)
        
        # ç¿»è¨³è¨­å®šã‚’å–å¾—
        translate_config = CONFIG.get("translate", {})
        translate_enabled = translate_config.get("enabled", False)
        show_translated = translate_config.get("show_translated", False)
        hide_original_when_translated = translate_config.get("hide_original_when_translated", False)

        for item, matched_keywords in entries:
            title = item["title"]
            url = item["link"]
            summary = item["summary"]
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã®å®‰å…¨ãªå‡¦ç†
            safe_title = clean_text_for_slack(title)
            if len(safe_title) > 2800:
                safe_title = safe_title[:2800] + "..."
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã¨URLã®çµ„ã¿åˆã‚ã›
            title_text = f"*<{url}|{safe_title}>*"
            if len(title_text) > 3000:
                title_text = f"*{safe_title}*\n<{url}|è«–æ–‡ã‚’èª­ã‚€>"
            
            title_text = clean_text_for_slack(title_text)
            blocks.append({"type": "section", "text": {"type": "mrkdwn", "text": title_text}})
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¡¨ç¤º
            if show_keywords and matched_keywords:
                try:
                    clean_keywords = [kw.replace('\\', '') for kw in matched_keywords]
                    keywords_text = "*ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:* " + ", ".join(clean_keywords)
                    
                    if len(keywords_text) > 3000:
                        keywords_text = keywords_text[:2800] + "..."
                    
                    keywords_text = clean_text_for_slack(keywords_text)
                    blocks.append({"type": "section", "text": {"type": "mrkdwn", "text": keywords_text}})
                except Exception as e:
                    print(f"[WARN] Error processing keywords for {item['id']}: {e}")
                    blocks.append({"type": "section", "text": {"type": "mrkdwn", "text": "*ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:* ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ"}})
            
            # æ¦‚è¦è¡¨ç¤ºï¼ˆç¿»è¨³è¡¨ç¤ºæ™‚ã¯è‹±èªã®abstractã‚’éè¡¨ç¤ºã«ã™ã‚‹ï¼‰
            if show_abstract and not (translate_enabled and show_translated and hide_original_when_translated):
                abstract_text = summary[:2800] + "..." if len(summary) > 2800 else summary
                safe_abstract = clean_text_for_slack(abstract_text)
                blocks.append({"type": "section", "text": {"type": "mrkdwn", "text": f"*Abstract:* {safe_abstract}"}})
            
            # ç¿»è¨³è¡¨ç¤º
            if translate_enabled and show_translated:
                try:
                    translated_summary = maybe_translate(summary)
                    translated_text = translated_summary[:2800] + "..." if len(translated_summary) > 2800 else translated_summary
                    safe_translated = clean_text_for_slack(translated_text)
                    blocks.append({"type": "section", "text": {"type": "mrkdwn", "text": f"*Abstract(ç¿»è¨³):* {safe_translated}"}})
                except Exception as e:
                    print(f"[WARN] Translation failed for {item['id']}: {e}")
            
            # ãƒ¡ã‚¿æƒ…å ±
            blocks.append({
                "type": "context",
                "elements": [
                    {
                        "type": "mrkdwn",
                        "text": f"`{item['id']}`  â€¢  published: {item['published']}",
                    }
                ],
            })
            
            blocks.append({"type": "divider"})
        
        # ãƒ–ãƒ­ãƒƒã‚¯ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
        valid_blocks = []
        for i, block in enumerate(blocks):
            try:
                if "type" not in block:
                    print(f"[WARN] Block {i} missing 'type' field: {block}")
                    continue
                
                valid_types = ["header", "section", "context", "divider"]
                if block["type"] not in valid_types:
                    print(f"[WARN] Block {i} has invalid type '{block['type']}': {block}")
                    continue
                
                if block["type"] in ["section", "context"] and "text" in block:
                    text_block = block["text"]
                    if "text" not in text_block or not text_block["text"]:
                        print(f"[WARN] Block {i} has empty text: {block}")
                        continue
                    
                    if len(text_block["text"]) > 3000:
                        print(f"[WARN] Block {i} text too long, truncating: {len(text_block['text'])} chars")
                        text_block["text"] = text_block["text"][:2800] + "..."
                
                if block["type"] == "header" and "text" in block:
                    header_text = block["text"]
                    if "text" not in header_text or not header_text["text"]:
                        print(f"[WARN] Block {i} header has empty text: {block}")
                        continue
                
                valid_blocks.append(block)
                
            except Exception as e:
                print(f"[WARN] Error validating block {i}: {e}, block: {block}")
                continue
        
        return valid_blocks
        
    except Exception as e:
        print(f"[ERROR] Error creating Slack blocks: {e}")
        return [
            {
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": f"*arXiv æ–°ç€è«–æ–‡é€šçŸ¥*\nã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
                }
            }
        ]

def make_no_papers_message() -> List[Dict[str, Any]]:
    """è«–æ–‡ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆ"""
    blocks: List[Dict[str, Any]] = []
    
    # æ—¥ä»˜ã®å®‰å…¨ãªå‡¦ç†
    try:
        date_str = NOW_LOCAL.strftime('%Y-%m-%d')
    except Exception:
        date_str = "ä»Šæ—¥"
    
    header = {
        "type": "header",
        "text": {
            "type": "plain_text",
            "text": f"arXiv ã§å…¬é–‹ã•ã‚ŒãŸæ–°ç€è«–æ–‡ ({date_str})",
            "emoji": True,
        },
    }
    blocks.append(header)
    
    # è«–æ–‡ãŒè¦‹ã¤ã‹ã‚‰ãªã„æ—¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    message_block = {
        "type": "section",
        "text": {
            "type": "mrkdwn",
            "text": "ğŸ“­ *æ–°ç€è«–æ–‡ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ*"
        }
    }
    blocks.append(message_block)
    
    # è¨­å®šæƒ…å ±ã‚’è¡¨ç¤º
    cats = CONFIG.get("categories", ["cs.CV"])
    keywords = CONFIG.get("keywords", [])
    hours_back = CONFIG.get("search", {}).get("hours_back", 24)
    
    config_info = f"*è¨­å®šæƒ…å ±:*\nâ€¢ ã‚«ãƒ†ã‚´ãƒª: {', '.join(cats)}\nâ€¢ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(keywords) if keywords else 'ãªã—'}\nâ€¢ æ¤œç´¢æ™‚é–“: éå»{hours_back}æ™‚é–“"
    
    config_block = {
        "type": "section",
        "text": {
            "type": "mrkdwn",
            "text": config_info
        }
    }
    blocks.append(config_block)
    
    blocks.append({"type": "divider"})
    
    return blocks


# ==============================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ==============================
def main() -> None:
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    try:
        # è¨­å®šã®å–å¾—
        categories = CONFIG.get("categories", ["cs.CV"])
        kw_patterns = compile_kw_patterns(CONFIG.get("keywords", []))
        max_posts = int(CONFIG.get("max_posts", 20))

        print(f"[INFO] Fetching papers from arXiv categories: {categories}")
        items = fetch_arxiv(categories, max_results=200)

        if not items:
            print("[ERROR] No papers fetched from arXiv")
            return

        # é–¢é€£è«–æ–‡ã®é¸æŠï¼ˆmax_postsã®åˆ¶é™ã‚’é©ç”¨ï¼‰
        selected = select_by_relevance(items, kw_patterns, max_posts=max_posts)
        
        if not selected:
            print("[INFO] No new papers matched criteria")
            blocks = make_no_papers_message()
            post_to_slack_webhook(blocks)
            print("[INFO] Posted 'no papers found' message to Slack")
            return

        # è«–æ–‡ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆ
        # ç·ä»¶æ•°ã‚’è¨ˆç®—ï¼ˆmax_postsã‚’è¶…ãˆã‚‹å ´åˆã®è¡¨ç¤ºç”¨ï¼‰
        hours_back = CONFIG.get("search", {}).get("hours_back", 24)
        all_matched = [item for item in items if item["id"] not in SEEN and within_search_hours(item["published"], hours_back)]
        total_matched = len(all_matched)
        
        # å®Ÿéš›ã«è¡¨ç¤ºã•ã‚Œã‚‹ä»¶æ•°ï¼ˆmax_postsã§åˆ¶é™ã•ã‚ŒãŸä»¶æ•°ï¼‰
        displayed_count = len(selected)
        
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’å‡ºåŠ›
        print(f"[DEBUG] Total matched papers: {total_matched}")
        print(f"[DEBUG] Papers selected for display: {displayed_count}")
        print(f"[DEBUG] Max posts setting: {max_posts}")
        
        blocks = make_slack_blocks(selected, total_count=total_matched, displayed_count=displayed_count)
        post_to_slack_webhook(blocks)

        # æ—¢èª­IDã‚’ä¿å­˜
        for item, _ in selected:
            SEEN.add(item["id"])
        SEEN_PATH.write_text(json.dumps(sorted(SEEN)), encoding="utf-8")
        
        print(f"[INFO] Posted {len(selected)} papers to Slack")
        
    except Exception as e:
        print(f"[ERROR] Main process failed: {e}")
        raise


if __name__ == "__main__":
    main()
